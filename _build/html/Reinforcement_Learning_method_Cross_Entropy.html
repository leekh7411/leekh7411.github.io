

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>One of the RL(Reinforcement Learning) method - Cross Entropy &mdash; RESEARCH.MD 1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Bellman equation of optimality" href="The_Bellman_Equation_of_Optimality.html" />
    <link rel="prev" title="A Brief Review of Probability Theory" href="A_Brief_Review_of_Probability_Theory.html" /> 
<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script>



  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> RESEARCH.MD
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="RNA_Secondary_Structures_01.html">RNA Secondary Structures (1/7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="RNA_Secondary_Structures_02.html">RNA Secondary Structures (2/7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="RNA_Secondary_Structures_03.html">RNA Secondary Structures (3/7)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Markow_Decision_Process_For_RNA_SSP_01.html">Markov Decision Process For Predict RNA Secondary Structure #1</a></li>
<li class="toctree-l1"><a class="reference internal" href="Markow_Decision_Process_For_RNA_SSP_02.html">Markov Decision Process For Predict RNA Secondary Structure #2</a></li>
<li class="toctree-l1"><a class="reference internal" href="About_The_Markov_Decision_Process.html">About the Markov Decision Process(MDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Brief_Review_of_Probability_Theory.html">A Brief Review of Probability Theory</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">One of the RL(Reinforcement Learning) method - Cross Entropy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#taxonomy-of-rl-methods">2. Taxonomy of RL methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-free">Model-Free</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-based">Model-Based</a></li>
<li class="toctree-l3"><a class="reference internal" href="#policy-based">Policy-Based</a></li>
<li class="toctree-l3"><a class="reference internal" href="#value-based">Value-Based</a></li>
<li class="toctree-l3"><a class="reference internal" href="#on-policy">On-Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#off-policy">Off-Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-entropy">Cross-entropy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#practical-cross-entropy">3. Practical cross-entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">4. Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="The_Bellman_Equation_of_Optimality.html">The Bellman equation of optimality</a></li>
<li class="toctree-l1"><a class="reference internal" href="Value_of_Action_Q_function.html">Why we use the Q-function?</a></li>
<li class="toctree-l1"><a class="reference internal" href="the_value_iteration_method.html">The value iteration method</a></li>
<li class="toctree-l1"><a class="reference internal" href="deep_q_nets.html">Deep Q-Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Policy_Gradients.html">Policy Gradients(PG)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure_Similarity_in_Graphs.html">Graph Structure’s Similarity methods</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RESEARCH.MD</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>One of the RL(Reinforcement Learning) method - Cross Entropy</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Reinforcement_Learning_method_Cross_Entropy.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p><em><strong>Written by LeeKH</strong></em></p>
<div class="section" id="one-of-the-rl-reinforcement-learning-method-cross-entropy">
<h1>One of the RL(Reinforcement Learning) method - Cross Entropy<a class="headerlink" href="#one-of-the-rl-reinforcement-learning-method-cross-entropy" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>많이 알려진 Deep Q-Network나 Advantage Actor-Critic처럼 주로 사용되는 방법론은 아니지만 Cross Entropy 나름의 장점이 있으므로 소개할만한 가치가 있다</li>
<li>Cross entropy 방법은 크게 2가지 장점을 갖고 있다<ul>
<li>Simplicity : 방법 자체가 쉽고 간단해서 직접 코드로 작성해도 라인 수가 많지 않음</li>
<li>Good Convergence :  강화학습의 환경(environment)가 간단하거나 episode가 짧고 보상(reward)이 빈번한 문제의 경우 꽤 잘 먹히는 장점이 있다. 물론 그런 문제들이 많진 않으나 알아서 손해볼 것은 없음</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="taxonomy-of-rl-methods">
<h2>2. Taxonomy of RL methods<a class="headerlink" href="#taxonomy-of-rl-methods" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>강화학습(RL)의 method들은 크게 model-based, model-free의 두가지로 나뉘어진다</li>
<li>cross-entropy는 model-free, policy-based에 속한다</li>
<li>모든 강화학습(RL)의 방법들은 아래와 같이 구분된다<ul>
<li>Model-free 또는 Model-based</li>
<li>Value-based 또는 Policy-based</li>
<li>On-policy 또는 Off-policy</li>
</ul>
</li>
</ul>
<div class="section" id="model-free">
<h3>Model-Free<a class="headerlink" href="#model-free" title="Permalink to this headline">¶</a></h3>
<p>model-free method들은 환경이 주는 영향에 대해서 크게 신경 쓰지 않고 학습한다. 이는 곧 환경(environment)이나 보상(reward)에 대한 별다른 모델을 설정하지 않고, 즉 신경 쓰지 않고, 학습하는 것으로 볼 수 있다</p>
<p>보통 환경이 영향을 준다고 보는 것을 예로 들면 빙판길을 걷는 로봇이 앞으로 이동하는 행동(action)을 취했는데 바람이 불어 이상한 곳(next-state but not expected)으로 가는 상황정도로 볼 수 있다</p>
<p>해당 방법은 환경에 대한 모델을 설정하기 어려운 복잡한 환경에 직면했을때 사용하는 방법이다</p>
</div>
<div class="section" id="model-based">
<h3>Model-Based<a class="headerlink" href="#model-based" title="Permalink to this headline">¶</a></h3>
<p>반대로  model-based는 환경 및 보상에 대한 별도의 모델을 사용해서 다음 행동을 고려할 때 함께 판단하는 방법을 의미한다</p>
<p>해당 방법은 비교적 환경에 대한 모델을 설정하기 쉬운(ex. 보드게임) 환경에 적용하기 좋다</p>
</div>
<div class="section" id="policy-based">
<h3>Policy-Based<a class="headerlink" href="#policy-based" title="Permalink to this headline">¶</a></h3>
<p>Agent가 다음 행동을 결정하는 것은 정책(Policy)를 기반으로 한다. policy-based 방법이란 agent의 정책을 직접적으로 근사시키는 방법을 의미한다. 이런 경우 정책은 다음에 수행해야 할 행동들에 대한 확률 값을 제공한다. Agent는 각 확률값을 기반으로 다음 행동을 결정하는 것이다</p>
</div>
<div class="section" id="value-based">
<h3>Value-Based<a class="headerlink" href="#value-based" title="Permalink to this headline">¶</a></h3>
<p>policy-based 방법은 각 행동들의 확률 값들로 다음 행동을 결정하는 반면, value-based 방법은 각 행동에 대한 가치를 계산해서 가장 높은 가치를 갖는 행동을 선택하는 방법이다.</p>
</div>
<div class="section" id="on-policy">
<h3>On-Policy<a class="headerlink" href="#on-policy" title="Permalink to this headline">¶</a></h3>
<p>학습에 사용되는 정책(policy)과 실제로 행동하는 정책을 나누지 않는 방법들을 의미(ex. SARSA). 두 정책은 모두 같아야 한다</p>
</div>
<div class="section" id="off-policy">
<h3>Off-Policy<a class="headerlink" href="#off-policy" title="Permalink to this headline">¶</a></h3>
<p>학습에 사용되는 정책과 실제로 행동하는 정책을 나누어 학습을 진행 할 수 있는 방법들을 의미(ex. Q-learning). 과거 데이터로 학습된 정책을 행동을 수행하는 정책에 사용할 수 있다</p>
</div>
<div class="section" id="cross-entropy">
<h3>Cross-entropy<a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<p>cross-entropy 방법을 위 속성으로 분류하자면 아래와 같다</p>
<ul class="simple">
<li>model-free 방법이다. 환경에 대한 별도의 모델을 설정하지 않는다</li>
<li>policy-based 방법이다. Agent의 정책을 바로 근사시킨다</li>
<li>on-policy 방법이다. 환경으로 부터 얻은 현재 데이터를 통해 행동하고 학습한다</li>
</ul>
</div>
</div>
<div class="section" id="practical-cross-entropy">
<h2>3. Practical cross-entropy<a class="headerlink" href="#practical-cross-entropy" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>cross-entropy 방법은 policy-based이다. 이는 정책을 바로 근사시키는 것을 의미하며 어떤 비선형 함수(ex.인공 신경망)이 정책(policy)를 만들어낸다. 이러한 방법은 기본적으로 모든 관측 상황, 상태(observation, s)에서 agent가 해야할 행동을 알려준다</li>
</ul>
<p><img alt="_images/1549678700522.png" src="_images/1549678700522.png" /></p>
<ul class="simple">
<li>Practical cross-entropy에서 정책(policy)는 행동(agent)에 대한 확률 분포를 나타낸다</li>
<li>즉, 위 그림처럼 인공 신경망에 관측 데이터를 통과 시켜 곧장 다음 행동에 대한 확률 분포를 얻는데 이를 policy로 보는 격</li>
<li>이제 얻어진 확률 분포에서 random sampling을 시행하여 각 episode에 대한 행동, 보상의 시퀀스 데이터를 얻는다</li>
<li>각 보상에 대해서 discount factor를 1로 설정한다고 가정하자. 이는 각 episode의 reward를 단순히 전부 더함으로서 전체 return을 얻을 수 있다</li>
</ul>
<p><img alt="_images/1549678996147.png" src="_images/1549678996147.png" /></p>
<ul class="simple">
<li>위 그림과 같이 4개의 episode를 진행하였고 discount factor는 1이기 때문에 episode의 길이가 긴 경우 total reward가 큰 것으로 해석할 수 있다</li>
<li>이는 곧 좋지 않았던 episode를 선별하는 기준이 되며, agent는 좋았던 episode의 데이터만 가지고 학습을 하면 된다. 이것이 cross-entropy 방식의 핵심이다</li>
</ul>
<p><img alt="_images/PCE.png" src="_images/PCE.png" /></p>
<ul class="simple">
<li>위 그림은 cross-entropy 방법을 간략히 소개한 다이어그램이다</li>
<li>해당 방법을 5가지 단계로 나누면 아래와 같다</li>
</ul>
<ol class="simple">
<li>현재 모델 및 환경을 바탕으로 N번의 episode를 진행한다</li>
<li>각 episode의 total reward를 계산하고 좋은 episode를 결정하기 위한 total reward의 경계선을 결정한다</li>
<li>경계값 보다 작은 total reward를 가진 episode는 버린다</li>
<li>남은 episode를 사용해 학습을 진행한다. 남은 episode의 각 지점의 observation이 입력으로, action을 정답으로 신경망을 학습한다</li>
<li>1번 과정을 계속해서 반복한다</li>
</ol>
</div>
<div class="section" id="summary">
<h2>4. Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>cross-entropy 방법은 환경 및 보상에 대한 모델을 설정하지 않는 model-free method 다</li>
<li>cross-entropy 방법은 정책 기반 학습을 수행한다</li>
<li>정책(policy)을 행동에 대한 확률 분포 값으로 본다. 정책 계산을 위해 비선형 함수를 사용하는데 Deep RL에서는 인공신경망을 사용한다</li>
<li>Agent는 정책을 기반으로 random sampling해서 현재 관측 값에서 행동을 결정하고 environment와 상호작용하여 episode를 기록한다</li>
<li>discount factor 값을 결정하고 각 episode마다 total reward를 계산해서 좋은 episode들을 바탕으로 학습을 진행한다. 이를 반복한다</li>
<li>Monte-Carlo method에 속한다. Sampling을 통해 optimization하기 때문</li>
<li>정책의 distribution과 Target distribution 사이의 KL-divergence 수치에서 cross-entropy를 최소화 하는 방향으로 학습을 진행해가기 때문에 cross-entropy method로 불린다</li>
</ul>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Deep Reinforcement Learning Hands-On, Maxim Lapan, 2018</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="The_Bellman_Equation_of_Optimality.html" class="btn btn-neutral float-right" title="The Bellman equation of optimality" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="A_Brief_Review_of_Probability_Theory.html" class="btn btn-neutral" title="A Brief Review of Probability Theory" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Kwangho Lee

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
    

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>